{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/intake/source/discovery.py:136: FutureWarning: The drivers ['stac-catalog', 'stac-collection', 'stac-item'] do not specify entry_points and were only discovered via a package scan. This may break in a future release of intake. The packages should be updated.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import intake\n",
    "import util\n",
    "import warnings\n",
    "from cmip6_preprocessing.preprocessing import read_data\n",
    "import numpy as np\n",
    "from cmip6_preprocessing.preprocessing import rename_cmip6\n",
    "import gsw\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy.ma as ma\n",
    "nm = 12\n",
    "import xesmf as xe\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a45ed6548c140149a7c9979967f22c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.32.78.45:39439</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/0000-0003-4189-3928/proxy/8787/status' target='_blank'>/user/0000-0003-4189-3928/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.32.78.45:39439' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in CMIP6 data using intake_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary():\n",
    "    \"\"\"\n",
    "    Function to get the dictionary of models and ensemble members of the historical runs \n",
    "    that have all of siconc, so, and thetao\n",
    "    \n",
    "    Returns the dictionary, the appropriate intake-esm catalog and the list of models needed to pass\n",
    "    to the next function that gets the datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('opening intake-esm catalog...')\n",
    "    url = \"https://raw.githubusercontent.com/andrewpauling/cmip6hack-so-project/master/catalogs/pangeo-cmip6.json\"\n",
    "    col = intake.open_esm_datastore(url)\n",
    "    print('done')\n",
    "    \n",
    "    cat = col.search(experiment_id=['historical'], table_id=['SImon', 'Omon'],\n",
    "                 grid_label='gn')\n",
    "    \n",
    "    uni_dict = cat.unique(['source_id', 'experiment_id', 'table_id', 'member_id'])\n",
    "    \n",
    "    cat = col.search(experiment_id=['historical'], table_id=['SImon', 'Omon'],\n",
    "                 grid_label='gn', variable_id=['siconc', 'thetao', 'so'])\n",
    "    \n",
    "    print('Find the models that have all three variables...')\n",
    "    models = set(uni_dict['source_id']['values']) # all the models\n",
    "\n",
    "    for table_id in ['SImon', 'Omon']:\n",
    "        if table_id == 'SImon':\n",
    "            query = dict(experiment_id='historical', table_id=table_id, \n",
    "                         variable_id='siconc', grid_label='gn')  \n",
    "            cat = col.search(**query)\n",
    "            models = models.intersection({model for model in cat.df.source_id.unique().tolist()})\n",
    "        else:\n",
    "            for variable_id in ['thetao', 'so']:\n",
    "                query = dict(experiment_id='historical', table_id=table_id, \n",
    "                             variable_id=variable_id, grid_label='gn')  \n",
    "                cat = col.search(**query)\n",
    "                models = models.intersection({model for model in cat.df.source_id.unique().tolist()})\n",
    "                \n",
    "    models = list(models)\n",
    "    print('Done')\n",
    "    \n",
    "    cat = col.search(experiment_id='historical', table_id=['Omon', 'SImon'], \n",
    "                 variable_id=['siconc', 'thetao', 'so'], grid_label='gn', source_id=models)\n",
    "    \n",
    "    print('Make sure all three variables have the same ensemble member...')\n",
    "    filt_dict = dict()\n",
    "\n",
    "    for model in models:\n",
    "        tmp2 = cat.search(source_id=model)\n",
    "        tmp2.df.head()\n",
    "        members = tmp2.df['member_id']\n",
    "        memlist = list()\n",
    "        for member in list(members):\n",
    "            a = tmp2.search(member_id=member, variable_id='siconc').df['activity_id'].empty\n",
    "            b = tmp2.search(member_id=member, variable_id='thetao').df['activity_id'].empty\n",
    "            c = tmp2.search(member_id=member, variable_id='so').df['activity_id'].empty\n",
    "            if not a and not b and not c and member not in memlist:\n",
    "                memlist.append(member)\n",
    "        filt_dict[model] =  memlist\n",
    "        \n",
    "    print('Done')\n",
    "    \n",
    "    return filt_dict, cat, models\n",
    "def get_datasets(filt_dict, cat, models):\n",
    "    \"\"\"\n",
    "    Function to load the dataset dictionaries for each of the variables siconc, so, thetao. Takes in the output\n",
    "    of get_dictionary()\n",
    "    Returns the dataset dictonary for each variable. Separate one for each variable due to problems with intake-esm \n",
    "    for some models\n",
    "    \"\"\"\n",
    "        \n",
    "    icedict = dict()\n",
    "    sodict = dict()\n",
    "    thetaodict = dict()\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        tmpice = cat.search(source_id=model, member_id=filt_dict[model], variable_id='siconc')\n",
    "        tmpdict_ice = tmpice.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': True},\n",
    "                                             cdf_kwargs={'chunks': {}, 'decode_times': True})\n",
    "        icedict.update(tmpdict_ice)\n",
    "        tmpice = None\n",
    "        tmpdictice = None\n",
    "    \n",
    "        tmpso = cat.search(source_id=model, member_id=filt_dict[model], variable_id='so')\n",
    "        tmpdict_so = tmpso.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': True},\n",
    "                                           cdf_kwargs={'chunks': {}, 'decode_times': True})\n",
    "        sodict.update(tmpdict_so)\n",
    "        tmpso = None\n",
    "        tmpdictso = None\n",
    "    \n",
    "        tmpthetao = cat.search(source_id=model, member_id=filt_dict[model], variable_id='thetao')\n",
    "        tmpdict_thetao = tmpthetao.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': True},\n",
    "                                                   cdf_kwargs={'chunks': {}, 'decode_times': True})\n",
    "        thetaodict.update(tmpdict_thetao)\n",
    "        tmpthetao = None\n",
    "        tmpdictthetao = None\n",
    "    \n",
    "    return icedict, sodict, thetaodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening intake-esm catalog...\n",
      "done\n",
      "Find the models that have all three variables...\n",
      "Done\n",
      "Make sure all three variables have the same ensemble member...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "[filt_dict, cat, models] = get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seemed tp have problems with this\n",
    "pickle.dump( [filt_dict, cat, models] , open( \"save.p\", \"wb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[filt_dict, cat, models] = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-CM4\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "SAM0-UNICON\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "IPSL-CM6A-LR\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "UKESM1-0-LL\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "CNRM-ESM2-1\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "HadGEM3-GC31-LL\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "CAMS-CSM1-0\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "EC-Earth3-Veg\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "MIROC6\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "CESM2\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "CanESM5\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "NESM3\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "MIROC-ES2L\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "CNRM-CM6-1\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 1 group(s)\n"
     ]
    }
   ],
   "source": [
    "icedict, sodict, thetaodict = get_datasets(filt_dict, cat, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# one model has inconsistencies b/w time grid so remove it\n",
    "sodict.pop('CMIP.CAMS.CAMS-CSM1-0.historical.Omon.gn')\n",
    "thetaodict.pop('CMIP.CAMS.CAMS-CSM1-0.historical.Omon.gn')\n",
    "icedict.pop('CMIP.CAMS.CAMS-CSM1-0.historical.SImon.gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# MIROC is weird - some NaNs at funny places in the ocean that\n",
    "# mean we can't calculate the bottom properties, so exclude\n",
    "sodict.pop('CMIP.MIROC.MIROC-ES2L.historical.Omon.gn')\n",
    "thetaodict.pop('CMIP.MIROC.MIROC-ES2L.historical.Omon.gn')\n",
    "icedict.pop('CMIP.MIROC.MIROC-ES2L.historical.SImon.gn')\n",
    "sodict.pop('CMIP.MIROC.MIROC6.historical.Omon.gn')\n",
    "thetaodict.pop('CMIP.MIROC.MIROC6.historical.Omon.gn')\n",
    "icedict.pop('CMIP.MIROC.MIROC6.historical.SImon.gn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify variable names and select first ensemble member\n",
    "for i in thetaodict:\n",
    "    thetaodict[i] = rename_cmip6(thetaodict[i]).isel(member_id = 0)\n",
    "    sodict[i] = rename_cmip6(sodict[i]).isel(member_id = 0)\n",
    "for i in icedict:\n",
    "    icedict[i] = rename_cmip6(icedict[i]).isel(member_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_props = {}\n",
    "for i in thetaodict:    \n",
    "    \n",
    "    # calculate cthetao and add to ds\n",
    "    cthetao = xr.apply_ufunc(gsw.CT_from_pt, sodict[i].so, thetaodict[i].thetao, dask='parallelized',\n",
    "                                         output_dtypes=[float,]).rename('cthetao').to_dataset() \n",
    "    \n",
    "    # calculate sigma0 and sigma2\n",
    "    sigma0 = xr.apply_ufunc(gsw.density.sigma0,sodict[i].so, cthetao.cthetao, dask='parallelized', \n",
    "                        output_dtypes=[float, ]).rename('sigma0').to_dataset()                                                                          \n",
    "   \n",
    "    sigma2=xr.apply_ufunc(gsw.density.sigma2,sodict[i].so, cthetao.cthetao, dask='parallelized', \n",
    "                        output_dtypes=[float, ]).rename('sigma2').to_dataset()\n",
    "    \n",
    "    # interpolate density data\n",
    "    surf_dens = sigma0.sigma0.interp(lev=10)\n",
    "    \n",
    "     # Calculate mixed layer depth based on density difference from 10m\n",
    "    dens_diff = sigma0.sigma0 - surf_dens\n",
    "    dens_diff = dens_diff.where(dens_diff > 0.03)\n",
    "    mld = dens_diff.lev.where(dens_diff==dens_diff.min(['lev'])).max(['lev']).rename('mld')\n",
    "    \n",
    "     # get variables at bottom\n",
    "    test = sigma0.sigma0 + sigma0.lev\n",
    "    bottom_depth = sigma2.lev.where(test == test.max(dim='lev')).max(dim='lev').rename('bottom_depth')\n",
    "    bottom_sigma2 = sigma2.sigma2.where(test == test.max(['lev'])).max(dim='lev').rename('bottom_sigma2')\n",
    "    bottom_temp = thetaodict[i].thetao.where(test == test.max(['lev'])).max(dim='lev').rename('bottom_temp')\n",
    "    bottom_salt = sodict[i].so.where(test==test.max(['lev'])).max(['lev']).rename('bottom_salt')\n",
    "    \n",
    "    relative_depth = (mld/bottom_depth).rename('relative_depth')\n",
    "    \n",
    "    calc_props[i] = xr.merge([bottom_depth, bottom_sigma2, bottom_temp, bottom_salt, relative_depth, mld])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/nputils.py:223: RuntimeWarning: All-NaN slice encountered\n",
      "  result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/nputils.py:223: RuntimeWarning: All-NaN slice encountered\n",
      "  result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/nputils.py:223: RuntimeWarning: All-NaN slice encountered\n",
      "  result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/core/nputils.py:223: RuntimeWarning: All-NaN slice encountered\n",
      "  result = getattr(npmodule, name)(values, axis=axis, **kwargs)\n"
     ]
    }
   ],
   "source": [
    " ## Observations \n",
    "# Read WOA using opendap\n",
    "Temp_url = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WOA/WOA13/1_deg/annual/temp'\n",
    "Salt_url = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WOA/WOA13/1_deg/annual/salt'\n",
    "#Oxy_url = 'http://apdrc.soest.hawaii.edu:80/dods/public_data/WOA/WOA13/5_deg/annual/doxy'\n",
    "\n",
    "ds_obs = xr.merge([\n",
    "    xr.open_dataset(Temp_url).tmn.load(),\n",
    "    xr.open_dataset(Salt_url).smn.load()])\n",
    "\n",
    "cthetao_obs = xr.apply_ufunc(gsw.CT_from_pt, ds_obs.smn, ds_obs.tmn, dask='parallelized',\n",
    "                                             output_dtypes=[float,]).rename('cthetao_obs').to_dataset() \n",
    "pdens_obs = xr.apply_ufunc(gsw.density.sigma2, ds_obs.smn, cthetao_obs.cthetao_obs, dask='parallelized',\n",
    "                                             output_dtypes=[float,]).rename('pdens_obs').to_dataset()\n",
    "\n",
    "ds_obs = xr.merge([ds_obs, cthetao_obs, pdens_obs])\n",
    "\n",
    "temp_obs = ds_obs.pdens_obs + ds_obs.lev\n",
    "    \n",
    "bottom_dens_obs = ds_obs.pdens_obs.where(temp_obs==temp_obs.max(['lev'])).max(['lev']).isel(time=-1)\n",
    "bottom_temp_obs = ds_obs.tmn.where(temp_obs==temp_obs.max(['lev'])).max(['lev']).isel(time=-1)\n",
    "bottom_salt_obs = ds_obs.smn.where(temp_obs==temp_obs.max(['lev'])).max(['lev']).isel(time=-1)\n",
    "topo_obs = (temp_obs- ds_obs.pdens_obs).isel(time=-1).max(['lev']).rename('topo')\n",
    "\n",
    "bottom_props_obs = xr.merge([bottom_dens_obs, bottom_temp_obs, bottom_salt_obs, topo_obs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get grid information from observations to regrid\n",
    "[lat_obs, lon_obs] = np.meshgrid(bottom_dens_obs.lat, bottom_dens_obs.lon)\n",
    "ds_out = xr.Dataset({'lat': (['x','y'],lat_obs),\n",
    "                     'lon': (['x','y'],lon_obs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuse existing file: nearest_s2d_1080x1440_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_384x320_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_332x362_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_330x360_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_294x362_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_330x360_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_292x362_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_384x320_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_291x360_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_292x362_360x180_peri.nc\n",
      "Reuse existing file: nearest_s2d_294x362_360x180_peri.nc\n"
     ]
    }
   ],
   "source": [
    "# regrid models to observation grid\n",
    "regridded_props = {}\n",
    "\n",
    "for i in calc_props:\n",
    "    ds_in = xr.Dataset({'lat': (['x','y'],thetaodict[i].lat),\n",
    "                        'lon': (['x','y'],thetaodict[i].lon)})\n",
    "    \n",
    "     # Regrid the data\n",
    "    f = xe.Regridder(ds_in, ds_out, np.str('nearest_s2d'), periodic=True, reuse_weights=True)\n",
    "    bottom_sigma2_regridded = f(calc_props[i].bottom_sigma2)\n",
    "    relative_depth_regridded = f(calc_props[i].relative_depth)\n",
    "    mld_regridded = f(calc_props[i].mld)\n",
    "\n",
    "    regridded_props[i] = xr.merge([bottom_sigma2_regridded, relative_depth_regridded, mld_regridded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give obs the same lat/lon names\n",
    "bottom_props_obs = bottom_props_obs.rename({'lat':'y','lon':'x'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.protocol.core - CRITICAL - Failed to Serialize\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 44, in dumps\n",
      "    for key, value in data.items()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 45, in <dictcomp>\n",
      "    if type(value) is Serialize\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 210, in serialize\n",
      "    raise TypeError(msg, str(x)[:10000])\n",
      "TypeError: ('Could not serialize object of type SubgraphCallable.', 'subgraph_callable')\n",
      "distributed.comm.utils - ERROR - ('Could not serialize object of type SubgraphCallable.', 'subgraph_callable')\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/comm/utils.py\", line 29, in _to_frames\n",
      "    msg, serializers=serializers, on_error=on_error, context=context\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 44, in dumps\n",
      "    for key, value in data.items()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 45, in <dictcomp>\n",
      "    if type(value) is Serialize\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 210, in serialize\n",
      "    raise TypeError(msg, str(x)[:10000])\n",
      "TypeError: ('Could not serialize object of type SubgraphCallable.', 'subgraph_callable')\n",
      "distributed.batched - ERROR - Error in batched write\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/batched.py\", line 93, in _background_send\n",
      "    payload, serializers=self.serializers, on_error=\"raise\"\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 227, in write\n",
      "    context={\"sender\": self._local_addr, \"recipient\": self._peer_addr},\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/comm/utils.py\", line 37, in to_frames\n",
      "    res = yield offload(_to_frames)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/utils.py\", line 1370, in offload\n",
      "    return (yield _offload_executor.submit(fn, *args, **kwargs))\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/concurrent/futures/_base.py\", line 425, in result\n",
      "    return self.__get_result()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/comm/utils.py\", line 29, in _to_frames\n",
      "    msg, serializers=serializers, on_error=on_error, context=context\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 44, in dumps\n",
      "    for key, value in data.items()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/core.py\", line 45, in <dictcomp>\n",
      "    if type(value) is Serialize\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in serialize\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 167, in <listcomp>\n",
      "    for obj in x\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 210, in serialize\n",
      "    raise TypeError(msg, str(x)[:10000])\n",
      "TypeError: ('Could not serialize object of type SubgraphCallable.', 'subgraph_callable')\n"
     ]
    }
   ],
   "source": [
    "# load regridded variables into memory for the time period we want\n",
    "# this will make the figure plotting below quicker\n",
    "for i in thetaodict:\n",
    "       regridded_props[i].isel(time=0).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(14,14))\n",
    "\n",
    "\n",
    "for n, i in enumerate(thetaodict):\n",
    "    ax = plt.subplot(4,3,n+2,projection = ccrs.SouthPolarStereo())\n",
    "    ax.set_extent([0.005, 360, -90, -50], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND,zorder=100,edgecolor='k')\n",
    "   \n",
    "    diff = regridded_props[i].mld\n",
    "    diff = ma.masked_where(lat_obs>0.,diff)\n",
    "    this = ax.pcolormesh(lon_obs,lat_obs,diff,transform=ccrs.PlateCarree(),vmin=50,vmax=500)\n",
    "\n",
    "       \n",
    "    plt.title(np.str(thetaodict[i].source_id))\n",
    "    fig.colorbar(this,ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('mld')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(14,14))\n",
    "\n",
    "ax = plt.subplot(4,3,1,projection = ccrs.SouthPolarStereo())\n",
    "ax.set_extent([0.005, 360, -90, -50], crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.LAND,zorder=100,edgecolor='k')\n",
    "     \n",
    "\n",
    "bottom_props_obs.pdens_obs.plot(cmap = plt.cm.viridis,\n",
    "                         transform=ccrs.PlateCarree(),vmin=36.8, vmax=37.2)\n",
    "ax.set_title('Observations')\n",
    "\n",
    "for n, i in enumerate(thetaodict):\n",
    "    ax = plt.subplot(4,3,n+2,projection = ccrs.SouthPolarStereo())\n",
    "    ax.set_extent([0.005, 360, -90, -50], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND,zorder=100,edgecolor='k')\n",
    "   \n",
    "    diff = bottom_props_obs.pdens_obs-regridded_props[i].bottom_sigma2\n",
    "    print(diff.shape)\n",
    "    diff.plot(cmap = plt.cm.RdBu_r,transform=ccrs.PlateCarree(),\n",
    "                         vmin=-.3, vmax=.3)\n",
    "    \n",
    "    regridded_props[i].mld.values = ma.masked_where(lat_obs>0.,regridded_props[i].mld.values)\n",
    "    regridded_props[i].mld.plot.contour(transform=ccrs.PlateCarree())\n",
    "       \n",
    "    plt.title(np.str(thetaodict[i].source_id))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('bottom_density_diff_polar_contour')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(14,14))\n",
    "\n",
    "ax = plt.subplot(4,3,1)\n",
    "\n",
    "bottom_props_obs.pdens_obs.isel(y=slice(0,12)).plot(cmap = plt.cm.viridis,\n",
    "                         vmin=36.8, vmax=37.2)\n",
    "ax.set_title('Observations')\n",
    "\n",
    "for n, i in enumerate(thetaodict):\n",
    "    ax = plt.subplot(4,3,n+2)\n",
    "    diff = bottom_props_obs.pdens_obs-regridded_props[i].bottom_sigma2\n",
    "    \n",
    "    diff.isel(y=slice(0,12)).plot(cmap = plt.cm.RdBu_r,\n",
    "                         vmin=-.3, vmax=.3)\n",
    "       \n",
    "    plt.title(np.str(thetaodict[i].source_id))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('bottom_density_diff')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure(figsize=(14,14))\n",
    "\n",
    "ax = plt.subplot(4,3,1,projection = ccrs.SouthPolarStereo())\n",
    "ax.set_extent([0.005, 360, -90, -50], crs=ccrs.PlateCarree())\n",
    "    \n",
    "\n",
    "bottom_props_obs.pdens_obs.plot(cmap = plt.cm.viridis,\n",
    "                         transform=ccrs.PlateCarree(),vmin=36.8, vmax=37.2)\n",
    "ax.set_title('Observations')\n",
    "\n",
    "for n, i in enumerate(thetaodict):\n",
    "    ax = plt.subplot(4,3,n+2,projection = ccrs.SouthPolarStereo())\n",
    "    ax.set_extent([0.005, 360, -90, -50], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND,zorder=100,edgecolor='k')\n",
    "   \n",
    "    diff = regridded_props[i].bottom_sigma2\n",
    "    print(diff.shape)\n",
    "    diff.plot(transform=ccrs.PlateCarree(),\n",
    "                         vmin=36.8, vmax=37.2)\n",
    "    \n",
    "    regridded_props[i].mld.plot.contour(transform=ccrs.PlateCarree())\n",
    "       \n",
    "    plt.title(np.str(thetaodict[i].source_id))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('bottom_density_abs_polar')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
